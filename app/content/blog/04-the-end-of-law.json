{
  "publishedAt": "17th Aug 2020",
  "title": "04 - The End of the Law",
  "tag": "General Concepts",
  "seoMetaImage": "https://blog.axbg.space/images/uploads/04_01.jpg",
  "seoDescription": "Do you like new computers?  \nI do too, even though I have to admit - choosing between different models or different components can be though.  ",
  "content": "Do you like new computers?  \r\n\r\nI do too, even though I have to admit - choosing between different models or different components can be though.  \r\n\r\nI'm pretty sure that, when you're ranking potential buy deals in your cart for either plug-n-play laptops of fancy battlestations, one of the criteria you first take into consideration is its CPU.  \r\n\r\n#\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_01.jpg' description='CPU close-up' alt='CPU close-up' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Photo by <a href='https://unsplash.com/@slavudin'>Slejven Djurakovic</a></p>\r\n</div>\r\n\r\n#\r\nIs it brand new or old-n-gold? \r\nWhat the hell are all those letters for?\r\nDoes it have a good price/quality ratio?\r\nIs it capable enough to shield you from slow running applications? \r\nIs it future-proof? \r\nHow many cores does it have?\r\nWhat frequency does it have?  \r\nIs it easily overclockable?  \r\n   \r\nThose are just some questions that came into my mind when I thought about this topic, but I'm sure that many more factors come into play when you're evaluating a computer's processor.  \r\n\r\nEither way, when we're estimating the value of such a product, even if you're a declared Intel fan or you've already joined the Ryzen hype, one thing is regarded as good: the bigger the number of cores, the better and, almost every time, the priciest. \r\n\r\nEven though a big number of cores sounds pretty badass and may convince you to drop a few more bucks to buy *that* fancy laptop, software-wise, the easiest approach to implement a solution is the sequential one - and this is both good news and bad news.  \r\n\r\nIn a sequential scenario, you run your carefully written instruction on a core that's pretty fast and pretty good at maths too. The problem in doing so is that the number of cores, no matter how big, loses its meaning: only one core will do the job, while the others won't be bothered by your code at all.  \r\n\r\nThe good news is that the hardware industry and, more specifically, the microprocessor building companies were guided - and sometimes challenged - since the beginning by a prediction, commonly known as a law, formulated by one of the Intel's cofounders - Gordon Moore. \r\n\r\nYes, we're talking about [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law#Moore's_second_law).  \r\n\r\n#\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_02.jpg' description=\"Moore's Law\" alt=\"Moore's Law\" >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Published on <a href='https://en.wikipedia.org/wiki/Moore%27s_law'>Wikipedia</a></p>\r\n</div>\r\n\r\n### A peculiar law\r\nFor software developers, this thing was more than welcomed: knowing that the brute processing power of a computer will double from 2 to 2 years meant that a program, no matter how slow it runs in the present, will become faster in the future without touching a line of its code.  \r\n\r\nIt's easy to remark that this mentality is not praised by code optimization gurus and, for all its worth, it shouldn't be viewed as a best practice, but despite those facts, its promises are too good to be ignored.  \r\n\r\nAll the effort poured into current-day software will generate future-proof solutions once the hardware will improve, simplifying the maintenance and update process of old codebases. \r\n\r\nThis sounds too good to be true, right?\r\nWell, it depends.  \r\n\r\nThe downfall of this approach is based on the assumption that Moore's Law is, indeed, law and not an empirical observation and we will continue to implement sequential solutions hoping that hardware will improve its performance in time. \r\n\r\nEven if we're good enough for now and the manufacturers are kinda keeping up with this atypical law, and have done so since 1965, it's safe/r to assume that the enthusiastic growth in raw processing power will start decreasing in the future.  \r\n\r\nThis is bad news and, arguably, a problem [we're not ready to face yet.](https://www.technologyreview.com/2020/02/24/905789/were-not-prepared-for-the-end-of-moores-law/) And it may not be that simple to see why.  \r\n\r\nFortunately, most professional tools make use of multithreading in a way of another, but their development is highly customized, thus complex, and, without perfectly executed QA, is prone to errors.  \r\n\r\nMore so, sequential code cannot be automatically parallelized. It's a process that requires a deep understanding of the modified system and how it can affect the expected output. On top of that, parallel computing is not a swiss army knife as some operations are not parallelizable at all.  \r\n\r\nEven if we assume that we would be able to run all of our complex operations in a parallel way, we hit the limits imposed by the [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law) which breaks our *parallelize everything!* utopia.  \r\n\r\nThis makes the future look a lot less optimistic, right?  \r\n\r\n**Well, not so fast.**  \r\n\r\nAnother good news is that, besides multicore processors that have dramatically increased, the number of computers has also skyrocketed in the last decades, making the distributed computing model another viable option that might help us to improve our solutions. (Spoiler: it already does)\r\n\r\nIt's safer to be slightly pessimistic when thinking about things that are not under our control, but this may not be the case when we're picturing technology's future. \r\n\r\nOne of the problems that can affect our capacity of prediction is the incapacity to predict breakthroughs: we are trying to project our current technological standards to a longer period than we should, ignoring the fact that something else may come from nowhere and change the way we live - just like computers did not so long ago.  \r\n\r\nConsidering our current use cases, a new technology that can process data 100 times faster than our computers may not seem like a big deal - but we should think about the opportunities we *cannot* think about, because they weren't invented yet. \r\n\r\nQuantum computing might be the future, but, until then, we should try to value our current resources as much as we can - including our multicore processors that, as programmers, we sometimes forget to capitalize on. \r\n\r\nLuckily, I was reading about *\"hardware's fatigue\"* around the same time I started a course on parallel and distributed computing as part of my master's degree. (what were the chances, right?) \r\n\r\nI know, sometimes coincidences are beautiful.  \r\n\r\nAs part of this course, I learned a little bit about implementing such systems and how they may help us overcome the sad demise of Moore's Law that may or may not happen sooner as we think - in case it didn't already happen. \r\n\r\nWe'll take 'em one by one to see if, in the end, parallel and distributed computing might be our saviors. \r\n\r\n### Parallel vs Distributed\r\nWe already clarified that when we're talking about regular *Hello worlds* we're talking about sequential computing. These implementations use a single core to get the job done and the evolution of hardware components will continue to increase their performance.  \r\n\r\nParallel and distributed computing are way different compared to the sequential approach, but they are also different from one another, even though the distinction can become blurry.  \r\n\r\nOn one hand, we have the parallel computing paradigm which implies the execution of a process using multiple processors at the same time. <sub><sup>(processor as in-unit that does the processing, not necessarily a CPU)</sup></sub>  \r\n\r\nThis can be easily achieved using the almost default multi-core processors in modern, mass-produced computers, but the same principle is valid for custom, high-performance systems used by companies or academia.\r\n\r\nOn the other hand, we have the distributed computing paradigm which has many form-factors, but, generally speaking, implies several computers sharing their resources to achieve a common goal. One of these computers is also the orchestrator who knows which data has to be processed, how and by whom.  \r\n\r\nBoth models imply that the processing is done at the same time, thus concurrently, a characteristic that could increase the speed of computation and, eventually, could help us to decrease the negative impact of a failing Moore's Law. \r\n\r\n#\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_03.jpg' description='Parallel computing' alt='Parallel computing' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Photo by <a href='https://pixabay.com/users/bodkins18-3428895/'>bodkins18</a></p>\r\n</div>\r\n\r\n#\r\n### Parallel Computing\r\nAs we noted before, parallel computing is a type of computation in which two or more units that act as a processor execute one or more tasks at the same time. Usually, the processors share a zone of memory which allows them to access the same data directly.  \r\n\r\nIf you're thinking that I just described a concurrent program, well, you're right - they are closely related, but sometimes is possible [to achieve one without the other](https://en.wikipedia.org/wiki/Parallel_computing). \r\n\r\nUnfortunately for my initial intention of writing a one-minute article, there's no easy way to approach this subject, which is, actually, a domain on its own, so we'll take small steps. \r\n\r\nHow can we start talking about parallel and concurrent computations better than talking about threads?\r\n\r\n### Threads\r\nSimply described, a thread is a small process running inside the actual process, having access to the same memory space. Behind the scenes, it's a lot of engineering implied that allow us to benefit of this abstraction, but for us, it's important to know that we can rely on threads when we don't want to block the main thread, which may be waiting for some inputs from a user, or we want to speed up things by fractioning a computationally intensive task in smaller chunks.  \r\n\r\nIf this topic sparked your interest, I recommend to take a look at [this book ](https://en.wikipedia.org/wiki/Modern_Operating_Systems)- it's big, it's complex and it exceeds the current topic, but it's a good read nonetheless.  \r\n\r\nFor today's examples, we'll use C/C++ because it's the closest we can get to the metal without leaving the field of high-level programming languages. Also, if we're talking about performance and tradition, there's no other contestant. \r\n\r\nInitially, I wanted to go for plain C, but, according to ANSI, a standard implementation for threads doesn't exist, even though [some](https://www.geeksforgeeks.org/multithreading-c-2/) is consistently used. Thus, I tried to rely on C methods when it was possible to do so without adding too much complexity that could affect readability. (that's why I used *new* instead of *malloc* and default namespaces)\r\n\r\nWith this being said, let's dig into threads!\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n\r\nusing namespace std;\r\n\r\nvoid doLoop(int n, const char* initiator) {\r\n    for (int i = 0; i < n; i++) {\r\n        printf(\"\\n%s iteration %d \\n\", initiator, i);\r\n        this_thread::sleep_for(chrono::milliseconds(200));\r\n    }\r\n}\r\n\r\nint main()\r\n{\r\n    int iterations = 10;\r\n\r\n    thread t = thread(doLoop, iterations, \"thread\");\r\n\r\n    doLoop(iterations, \"main_thread\");\r\n\r\n    t.join();\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_04.jpg' description='Threads Example' alt='Threads Example' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Threads Example</p>\r\n</div>\r\n\r\nAs you can see, threads are not a big deal - in a simple scenario. When multiple threads are implied and there's a serious amount of data that needs to be processed in parallel, things might get messy.  \r\n\r\nOne advantage of threads is that they can access the same memory space as their parent process, decreasing the time of passing the variables around to a practical 0. \r\n\r\nThis advantage is also one of the biggest threats, though, as threads can access a variable and modify it while another thread was expecting to find there the initial value.  \r\n\r\nAnother good thing is that operating systems do a marvelous job managing threads for our lazy asses - but, again, here's another problem: threads are anything, but foreseeable. You can start two threads at once, but still, be unable to call which one will finish first if their tasks have similar complexity.  \r\n\r\nThe word used to describe their behavior is nondeterministic and general good practice is to avoid using bare threads if your programming language of choice provides other ways to manage concurrent tasks. \r\n\r\nIn general, problems generated by an unexhaustive orchestration of threads fall under the **race condition** umbrella, in which a result is different than what was expected because a series of events happened in a different order than what was originally planned. \r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n\r\nusing namespace std;\r\n\r\nvoid changeValue(int& n, int newValue) {\r\n    n = newValue;\r\n}\r\n\r\nint main()\r\n{\r\n    int numberOfThreads = 10;\r\n    int value = 0;\r\n\r\n    thread* threads = new thread[numberOfThreads];\r\n\r\n    for (int i = 0; i < numberOfThreads; i++) {\r\n        threads[i] = thread(changeValue,ref(value), i);\r\n    }\r\n\r\n    for (int i = 0; i < numberOfThreads; i++) {\r\n        threads[i].join();\r\n    }\r\n\r\n    delete[] threads;\r\n\r\n    printf(\"Final result: %d\\n\", value);\r\n    return 0;\r\n}\r\n```\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_05.jpg' description=\"Race condition examples\" alt=\"Race condition examples\" >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Multiple race condition runs</p>\r\n</div>\r\n\r\nThe bigger problem with this kind of... behavior is that it can become almost impossible to detect - the system may work perfectly in each test, but fail shortly after.  \r\n\r\n**Not nice.**\r\n\r\nTo tame threads we need to use concepts like locks, semaphores, and mutexes which ensure, in a way or another, that a critical section - a block that contains common data to two or more threads - is never crossed by two threads at the same time. These concepts are very common in operating system implementations - one more reason to take a look at the fantastic book I talked about before!\r\n\r\nIn C/C++, the most basic way to synchronize two threads is through the usage of one or more mutexes. \r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n#include <mutex>\r\n\r\nusing namespace std;\r\n\r\nvoid firstChange(int &n, int newValue, mutex& mtx) {\r\n    mtx.lock();\r\n    this_thread::sleep_for(chrono::milliseconds(1000));\r\n    n = newValue;\r\n    mtx.unlock();\r\n}\r\n\r\nvoid secondChange(int& n, int newValue, mutex& mtx) {\r\n    mtx.lock();\r\n    n = newValue;\r\n    mtx.unlock();\r\n}\r\n\r\nint main()\r\n{\r\n    int value = 0;\r\n    mutex mtx;\r\n\r\n    thread t1 = thread(firstChange, ref(value), 5, ref(mtx));\r\n    thread t2 = thread(secondChange, ref(value), 10, ref(mtx));\r\n\r\n    t1.join();\r\n    t2.join();\r\n\r\n    printf(\"Final result %d\", value);\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_06.jpg' description='Mutex example' alt='Mutex example' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Mutex example</p>\r\n</div>\r\n\r\nEach time we run the code, we'll reach the same result. Cool. \r\nNow we know that our threads won't try to snitch and modify the variable at the same time.  \r\n\r\nImagine what will happen if our scenario would be a little bit more complex and more mutexes will be needed. More mutexes mean more secure code, right?  \r\nYes and no - and that's another fun part!\r\n\r\nIf two or more mutexes exist, the terrain is more than ready for another player to appear: meet **deadlock**.\r\nA deadlock is an unhappy scenario when two or more threads block each other by circularly waiting for locks. \r\n\r\nYou can take a quick look, but, be advised, it's pretty scary.\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_07.jpg' description='Deadlock' alt='Deadlock' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Published on <a href='https://en.wikipedia.org/wiki/Deadlock'>Wikipedia</a></p>\r\n</div>\r\n\r\nIt's even scarier if we try it ourselves and see that deadlocks are not something to joke about.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n#include <mutex>\r\n\r\nusing namespace std;\r\n\r\nvoid compute(mutex& mtx1, mutex& mtx2) {\r\n    mtx1.lock();\r\n    this_thread::sleep_for(chrono::milliseconds(200));\r\n\r\n   //  both threads are waiting to lock the second mutex\r\n   // which is already locked by the other thread\r\n    mtx2.lock();\r\n\r\n   // this line will never be printed\r\n    printf(\"both resources locked - finished\");\r\n\r\n    mtx2.unlock();\r\n    mtx1.unlock();\r\n}\r\n\r\n\r\nint main()\r\n{\r\n    int iterations = 10;\r\n\r\n    mutex mtx1, mtx2;\r\n\r\n    // we pass the mutexes in alternative order\r\n    thread t1 = thread(compute, ref(mtx1), ref(mtx2));\r\n    thread t2 = thread(compute, ref(mtx2), ref(mtx1));\r\n\r\n    t1.join();\r\n    t2.join();\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\nIn this example, the threads will never exit, because each one is waiting on a mutex that was already locked, and, as our main thread is waiting on them to join, it will remain stuck too. \r\n\r\nSadly, deadlocks can happen if we don't pay attention, but there are some [general rules](https://en.wikipedia.org/wiki/Deadlock_prevention_algorithms) that, if followed closely, should help in avoiding this ugly scenario.  \r\n\r\nBear in mind that, even with all the precautions taken, deadlocks can still happen, but operating systems are, in general, [capable of handling them](https://en.wikipedia.org/wiki/Deadlock#Deadlock_handling) without breaking a sweat.  \r\n\r\nNow that threads seem more approachable, is time for a *throwback to high school*.  \r\n\r\nI have to admit that, sometimes, I get pretty melancholic thinking that the days of living without knowing what a Jira ticket is are behind me, but that's not the reason why I would like to make a little trip back to the past.   \r\n\r\nAs for any topic, we need an example on which to apply our newly gathered knowledge and I'll assume that during your high school years you got pretty familiar with common divisors - more precisely with the *greatest common divisor*. \r\n\r\nYep, you guessed it, we'll be trying to implement the algorithm in a concurrent way to see if there's any benefit at all and if, spoilers ahead, some other unforeseen errors may occur. More specifically, we'll generate a random array of numbers and we'll compute the GCD for every two consecutive numbers we find there. \r\n\r\nWe'll also take the time and add a little benchmark to measure how good our solutions perform.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n#include <omp.h>\r\n\r\nusing namespace std;\r\n\r\nint* generateRandomNumbers(int n) {\r\n    if (n > 0) {\r\n        int* numbers = new int[n];\r\n\r\n        for (int i = 0; i < n; i++) {\r\n            numbers[i] = rand() + 1;\r\n        }\r\n\r\n        return numbers;\r\n    }\r\n\r\n    return NULL;\r\n}\r\n\r\nint gcd(int a, int b) {\r\n    int r = a % b;\r\n\r\n    while (r != 0) {\r\n        a = b;\r\n        b = r;\r\n        r = a % b;\r\n    }\r\n\r\n    return b;\r\n}\r\n\r\nvoid executeAndBenchmark(const char* name, void (*function)(int*, int), int* numbers, int length) {\r\n    auto start = chrono::system_clock::now();\r\n\r\n    function(numbers, length);\r\n\r\n    auto end = chrono::system_clock::now();\r\n    chrono::duration<double> elapsed = end - start;\r\n    printf(\"%s GCD finished in %f\\n\", name, elapsed.count());\r\n}\r\n\r\nvoid sequentialGcd(int* numbers, int length) {\r\n    int* results = new int[length / 2];\r\n\r\n    for (int i = 0, j = 0; i < length; i += 2, j++) {\r\n        results[j] = gcd(numbers[i], numbers[i + 1]);\r\n    }\r\n\r\n    delete[] results;\r\n}\r\n\r\nvoid threadsGcd(int* numbers, int length) {\r\n    int* firstHalf = new int[length / 2];\r\n    int* secondHalf = new int[length / 2];\r\n\r\n    for (int i = 0; i < length / 2; i++) {\r\n        firstHalf[i] = numbers[i];\r\n    }\r\n\r\n    for (int j = 0, i = length / 2; i < length; i++) {\r\n        secondHalf[j] = numbers[i];\r\n    }\r\n\r\n    // we can use futures to get a pointer to the results on each thread\r\n    // but it's not the scope of the example\r\n    thread t1 = thread(sequentialGcd, ref(firstHalf), length / 2);\r\n    thread t2 = thread(sequentialGcd, ref(secondHalf), length / 2);\r\n\r\n    t1.join();\r\n    t2.join();\r\n\r\n    delete[] firstHalf;\r\n    delete[] secondHalf;\r\n}\r\n\r\n\r\nint main()\r\n{\r\n    int length = 100000;\r\n    int* numbers = generateRandomNumbers(length);\r\n\r\n    executeAndBenchmark(\"Sequential\", sequentialGcd, numbers, length);\r\n    executeAndBenchmark(\"Threads\", threadsGcd, numbers, length);\r\n\r\n    delete[] numbers;\r\n    return 0;\r\n}\r\n```\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_08.jpg' description='Threads GCD' alt='Threads GCD' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Sequential & Parallel GCD</p>\r\n</div>\r\n\r\nAs you've probably expected, threads did a slightly better job, but it required a few additional lines of code. It's time to jump up a level and see if we can make use of threads without explicitly taking care of them. \r\n\r\n### Avoiding wheel's reinvention\r\nIn terms of achieving concurrency, threads are the building block of everything. Sometimes it's easier to use a thread to resolve a small task as it would be an overkill to use anything else, but it shouldn't be viewed as the safest solution.\r\n\r\nMost programming languages have complex abstractions built-in that can hide even the notion of threads and I think that, if you build a system that should be reliable before anything else, you should always use them. Also, there's a common saying regarding a wheel that states the obvious: you don't need to build anything by yourself. \r\n\r\nBeing pretty curious by nature, I don't feel pretty good when I can't affirm that I understand the tools I'm using, but magic is good from time to time - and this is available OpenMP. \r\n\r\nOpenMP, an acronym for Open Multi-Processing, is an open-source library that handles parallel programming in a big way. Built natively for C, C++, and Fortran OpenMP is a viable option for our solution up there to become smoother, hiding away the true complexity of concurrent processing and letting us live in a beautiful world where deadlocks and race conditions are distant problems. \r\n\r\n*(Disclaimer: I don't encourage you to use a higher level of abstraction if you don't understand what problem is solved by doing so - I would argue that digging deeper will make a more well-rounded programmer, even if you don't go all the way down to transistors - I surely didn't. **Yet**)*\r\n\r\nWith a little bit of rewriting, our little example becomes more precise and, hopefully, a little bit faster. \r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n#include <omp.h>\r\n\r\nusing namespace std;\r\n\r\nint* generateRandomNumbers(int n) {\r\n    if (n > 0) {\r\n        int* numbers = new int[n];\r\n\r\n        for (int i = 0; i < n; i++) {\r\n            numbers[i] = rand() + 1;\r\n        }\r\n\r\n        return numbers;\r\n    }\r\n\r\n    return NULL;\r\n}\r\n\r\nint gcd(int a, int b) {\r\n    int r = a % b;\r\n\r\n    while (r != 0) {\r\n        a = b;\r\n        b = r;\r\n        r = a % b;\r\n    }\r\n\r\n    return b;\r\n}\r\n\r\nvoid executeAndBenchmark(const char* name, void (*function)(int*, int), int* numbers, int length) {\r\n    auto start = chrono::system_clock::now();\r\n\r\n    function(numbers, length);\r\n\r\n    auto end = chrono::system_clock::now();\r\n    chrono::duration<double> elapsed = end - start;\r\n\r\n    printf(\"%s GCD finished in %f\\n\", name, elapsed.count());\r\n}\r\n\r\nvoid parallelGcd(int* numbers, int length) {\r\n    // we use all the available cores of our processor\r\n    omp_set_num_threads(omp_get_num_procs());\r\n\r\n    int* results = new int[length / 2];\r\n\r\n# pragma omp parallel for shared(numbers, results)\r\n    for (int i = 0; i < length; i += 2) {\r\n        results[i != 0 ? i / 2 : 0] = gcd(numbers[i], numbers[i + 1]);\r\n    }\r\n\r\n    delete[] results;\r\n}\r\n\r\nint main()\r\n{\r\n    int length = 100000;\r\n    int* numbers = generateRandomNumbers(length);\r\n\r\n    executeAndBenchmark(\"OMP\", parallelGcd, numbers, length);\r\n\r\n    delete[] numbers;\r\n    return 0;\r\n}\r\n```\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_09.jpg' description='OpenMP GCD' alt='OpenMP GCD' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">OpenMP GCD</p>\r\n</div>\r\n\r\nIn this scenario, OMP looks like a blessing! \r\n\r\nWe've covered one of the most used pragmas offered by OpenMP, but a lot more are available. If you want to learn more about it I recommend the [official docs](https://www.openmp.org/spec-html/5.0/openmp.html).  \r\n\r\nNow, what if I tell you that the performance can get a little bit better with just a tiny tweak? \r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_10.jpg' description='Secret sauce GCD' alt='Secret sauce GCD' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Secret sauce GCD</p>\r\n</div>\r\n\r\nIt's crazy, right? Let's take a look at the code to see what is different.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <thread>\r\n#include <omp.h>\r\n\r\nusing namespace std;\r\n\r\nint* generateRandomNumbers(int n) {\r\n    if (n > 0) {\r\n        int* numbers = new int[n];\r\n\r\n        for (int i = 0; i < n; i++) {\r\n            numbers[i] = rand() + 1;\r\n        }\r\n\r\n        return numbers;\r\n    }\r\n\r\n    return NULL;\r\n}\r\n\r\nint gcd(int a, int b) {\r\n    int r = a % b;\r\n\r\n    while (r != 0) {\r\n        a = b;\r\n        b = r;\r\n        r = a % b;\r\n    }\r\n\r\n    return b;\r\n}\r\n\r\nvoid parallelGcd(int* numbers, int length) {\r\n    // we use all the available cores of our processor\r\n    omp_set_num_threads(omp_get_num_procs());\r\n\r\n    int* results = new int[length / 2];\r\n    auto start = chrono::system_clock::now();\r\n\r\n# pragma omp parallel for shared(numbers, results)\r\n    for (int i = 0; i < length; i += 2) {\r\n        results[i != 0 ? i / 2 : 0] = gcd(numbers[i], numbers[i + 1]);\r\n    }\r\n\r\n    auto end = chrono::system_clock::now();\r\n    chrono::duration<double> elapsed = end - start;\r\n\r\n    printf(\"OMP GCD finished in %f\\n\", elapsed.count());\r\n\r\n    delete[] results;\r\n}\r\n\r\nvoid secretSauce(int* numbers, int length) {\r\n    omp_set_num_threads(omp_get_num_procs());\r\n\r\n    int** results = new int* [length / 2];\r\n    for (int i = 0; i < length / 2; i++) {\r\n        results[i] = new int[1000];\r\n    }\r\n\r\n    auto start = chrono::system_clock::now();\r\n\r\n# pragma omp parallel for shared(numbers, results)\r\n    for (int i = 0; i < length; i += 2) {\r\n        results[i != 0 ? i / 2 : 0][0] = gcd(numbers[i], numbers[i + 1]);\r\n    }\r\n\r\n    auto end = chrono::system_clock::now();\r\n    chrono::duration<double> elapsed = end - start;\r\n\r\n    printf(\"Secret sauce GCD finished in %f\\n\", elapsed.count());\r\n\r\n    for (int i = 0; i < length / 2; i++) {\r\n        delete[] results[i];\r\n    }\r\n\r\n    delete[] results;\r\n}\r\n\r\nint main()\r\n{\r\n    int length = 100000;\r\n    int* numbers = generateRandomNumbers(length);\r\n\r\n    parallelGcd(numbers, length);\r\n    secretSauce(numbers, length);\r\n\r\n    delete[] numbers;\r\n    return 0;\r\n}\r\n```\r\n\r\nWell, that's pretty appealing from a computational point of view, even if we stressed the memory a little bit more than before. I will be honest and I'll say that the results are not consistent: sometimes the tweak works like clockwork, other times it fails miserably. For a bigger input, our secret sauce lags a lot behind the purely concurrent solution.  \r\n\r\nYou may have also remarked that, in this scenario, I've benchmark only the actual computation, without the time consumed by allocating memory - which, in our previous cases was included. I did that because, in the context of our new shiny solution, memory allocation is a big deal.  \r\n\r\nWith everything considered, I don't think it's a gimmick you should consider implementing *out in the real world*, but, for the sake of introducing another topic, why does the result of our benchmark look even better in this particular *saucy* case? \r\n\r\nThat's the beauty of computers - you can always dig deeper!\r\n\r\n### Another type of memory\r\nLike we stated before, threads share the same memory space and are run concurrently. This means that they will probably use another core than our main program. They have access to the same memory and, from our CPU's perspective, our computer has only one module of RAM he needs to exchange data with, so, in a perfect world, this scenario would also be perfect. \r\n\r\nIn reality, CPUs are fast, blazing fast, but they need data and data - well - it's not so fast. Every time your CPU request some data from your RAM it has to wait until it receives that data and, for a CPU, waiting is bad, so bad that engineers thought of a solution to prevent this scenario from happening too often.  \r\n\r\nCPUs have their own memory, called cache, which is typically organized in 3 levels: the bigger the level, the slower the memory and the further away from the processor. Cache memory stores data that was recently required by CPU and brought from memory.  \r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_11.jpg' description='CPU Cache' alt='CPU Cache' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Published on <a href='https://en.wikipedia.org/wiki/Multi-core_processor'>Wikipedia</a></p>\r\n</div>\r\n\r\nL1 cache is the fastest type of memory in your computer, but, also, the smallest being placed inside the chip. Each core of your multicore chip has one and is consulted each time the processor requests new data. The request can result in a hit, case in which the processor doesn't have to wait because the data is already here, or a miss, thus entering the other cache levels.  \r\n\r\nL2 may or may not be shared, but higher caches are usually common to all the cores and they are larger and slower. In the most unfavorable scenario when the requested data is not present in neither of the caches, the RAM does its magic and the processor has to wait for data, which is usually stored in cache before reaching a core. \r\n\r\nThe thing that slowed our initial implementation is closely related to how the non-shared caches work, especially L1 and, sometimes L2. Even though they are independent pieces of hardware and can hold different data, when a common reference exists in two or more cores, they frequently synchronize to ensure that its core will receive the freshest data.  \r\n\r\nOur cores don't modify the same variables, this is true, but when a core requires data, RAM typically provides more than it was asked for, assuring that data which is contiguously stored will be available in the cache if (but most of the times when) the core requires it.  \r\n\r\nThis process is called **cache coherence** and it's really useful most of the time. \r\n\r\nIn our initial example, the structure that holds our results is, indeed, contiguous and each time a thread updates it, the synchronization process between caches takes place, affecting the performance in a very subtle way.  \r\n\r\nBy using a two-dimensional array, as we did in our best example to date, we tricked the cache coherence into never taking place. Each value of interest, stored at the beginning of each row, is further away in memory compared to the other values of interest and it's requested, thus stored in the cache, only by the core that needs to access it. \r\n\r\nA nice optimization technique, but, most of the time, cache coherence is useful - we just forced its capabilities for the sake of learning.  \r\n\r\nWith or without higher-level abstractions, parallel programming is powerful and, even if sometimes it can become more than a lot, it's worth it - but, for now, we'll give it a rest. \r\n\r\nDon't erase it from your memory, though, because we'll make use of it again soon enough!\r\n\r\n### Distributed Computing\r\n\r\n#\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_12.jpg' description='Distributed computing' alt='Distributed computing' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Photo by <a href='https://unsplash.com/@tvick'>Taylor Vick</a></p>\r\n</div>\r\n\r\n#\r\nIf the discussion about parallel computing seemed vast, think about distributed computing as something way more diverse and even tougher to define, because, before anything else, it's implying a network and has a lot of form factors.  \r\n\r\nTo keep it simple, we'll think about a distributed system as being composed of many independent nodes, each with its data and executing some processes to achieve a common goal.  \r\n\r\nDistributed systems are everywhere, starting from the client-server application that you've built during your networking class to the WWW itself, and it's pretty natural to conclude that two computers can process together more data than just one computer could. \r\n\r\nThe unpleasant part is that, besides the refactoring needed to transform a sequential program into one that operates in parallel, an effort that should be considered in parallel systems with shared memory as well, distributed computing also deals with the communication and synchronization of data. \r\n\r\nIf you want to synchronize the data between two nodes, you can even do it with bare sockets, without relying on application-level protocols, and, for what it's worth, it may not look like an unreasonable task. When the phenomenon of scaling becomes a necessity, though, it becomes tough the same way it happens with threads. \r\n\r\nAs you've probably guessed by now, under the umbrella of distributed computing resides other types of *computing* which may have a little bit more resonance: cloud computing, cluster computing, grid computing - they all face the same major challenges and new levels of abstraction were added to help the programmers work more efficiently. \r\n\r\nRemote method invocation, Remote procedure calls, HTTP calls: they are all magnificent concepts that can help you build a distributed system without thinking too much about the *distributed* part, but you should still take care about synchronization between nodes and pass data around, depending on which task you want to execute.\r\n\r\nFor our example, though, we will make use of a library that will help us distribute our program, the standard greatest common divisor we've done before, to multiple nodes without exceeding the bounds of writing a single program. \r\n\r\n### Enter OpenMPI\r\nEasy to be confused with our friend above, OpenMP, OpenMPI, standing for Open Message Passing Interface, is a library that implements the MPI standard and deals with communication and data transfer in parallel and distributed applications.  \r\n\r\nIt may seem like magic, but, under the hood, it's using the network to pass messages around between a central node, the only one that will explicitly run the application and organize the data, and the rest of them. If the context is suitable, though, the nodes can communicate directly without broadcasting their messages to the central node. \r\n\r\nBesides the [official documentation](https://www.open-mpi.org/doc/current/), I used [these tutorials](https://mpitutorial.com/tutorials/) to get comfortable with OpenMPI. In case the following examples get blurry or my explanations are not satisfactory enough, you should check them out. \r\n\r\nTo simplify the whole process compiling our source code, preparing nodes, and, finally, run the program, I've created this bash script - *clean and tidy*.  \r\n\r\n```bash\r\n#!/bin/bash\r\n\r\nif ! grep -q localhost nodes; then\r\n    echo \"localhost\" >> nodes\r\n    echo -en \"Added localhost as first node\"\r\nfi\r\n\r\nmpic++ Source.cpp -o source &>/dev/null\r\n\r\nmpirun -np 2 -hostfile nodes ./source\r\n```\r\n\r\nBefore running the script, though, you have to do some work by hand. \r\n\r\nFirst of all, make sure that you've installed OpenMPI on each node. If you use an Ubuntu-based machine as I did, you can use the following command:\r\n\r\n```bash\r\n# don't forget to \"apt update\" before - I do it every time..\r\nsudo apt install openmpi-bin openmpi-common libopenmpi2 libopenmpi-dev\r\n```\r\n\r\nAfter that, you have to compile the source code before actually executing the program. \r\n\r\nYou can do it manually or running the script once - ignoring the fact that it will fail to execute in a distributed manner. We're interested in the executable generated by the script and, also, in the nodes file which will be useful soon.  \r\n\r\nNow, you have to copy the executable on each node in the home directory of your user and change the permissions so the file can be executed. Something like *100* will be more than fine if you don't plan to use multiple users.  \r\n\r\nThen, you need to append the addresses of your nodes in your OS's hosts file and give them relevant names. In Linux, it will be placed under */etc*, in the *hosts* file. You have to do that only on the machine that will be used as the central node. \r\n\r\nSave the file and append the names associated with the addresses in the nodes file that was created by the script. 'localhost' should remain the first entry because this is, actually, the place where the current machine is marked as ~~master~~ main.  \r\n\r\nThe last step includes generating a ssh key on the central node and appending its public counterpart in each node's *~/.ssh/authorized_keys* file to obtain shell access. If you don't remember exactly how it's done, take a look [here](https://linuxize.com/post/how-to-set-up-ssh-keys-on-ubuntu-1804/) - it shouldn't take more than 2 minutes. \r\n\r\nAfter those steps, you just have to run the script once again and *voila* - you're good to go!\r\n\r\nNow, let's move on to the actual *kind of magic*.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <mpi.h>\r\n\r\nusing namespace std;\r\n\r\nint* generateRandomNumbers(int n) {\r\n    if (n > 0) {\r\n        int* numbers = new int[n];\r\n\r\n        for (int i = 0; i < n; i++) {\r\n            numbers[i] = rand() + 1;\r\n        }\r\n\r\n        return numbers;\r\n    }\r\n\r\n    return NULL;\r\n}\r\n\r\nint gcd(int a, int b) {\r\n    int r = a % b;\r\n\r\n    while (r != 0) {\r\n        a = b;\r\n        b = r;\r\n        r = a % b;\r\n    }\r\n\r\n    return b;\r\n}\r\n\r\nint* sequentialGcd(int* numbers, int length) {\r\n    int* results = new int[length / 2];\r\n\r\n    for (int i = 0, j = 0; i < length; i += 2, j++) {\r\n        results[j] = gcd(numbers[i], numbers[i + 1]);\r\n    }\r\n\r\n    return results;\r\n}\r\n\r\nint main(int argc, char** argv)\r\n{\r\n    // length should be divisible with the worldSize - the number of nodes\r\n    int worldRank, worldSize, length = 100000;\r\n    int* numbers, * results;\r\n\r\n    MPI_Init(&argc, &argv);\r\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\r\n    MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\r\n\r\n    int nodeLength = length / worldSize;\r\n\r\n    // because each array of inputs will result in an array half its size\r\n    int resultLength = length / 2;\r\n\r\n    // the random numbers will be generated only on the central node\r\n    if (worldRank == 0) {\r\n        numbers = generateRandomNumbers(length);\r\n        results = new int[resultLength];\r\n    }\r\n\r\n    int* receivedNumbers = new int[nodeLength];\r\n\r\n    // scatter the data to all of the nodes\r\n    MPI_Scatter(numbers, nodeLength, MPI_INT, receivedNumbers, nodeLength, MPI_INT, 0, MPI_COMM_WORLD);\r\n\r\n    // compute gcd for the received chunk of numbers\r\n    int* gcdResults = sequentialGcd(receivedNumbers, nodeLength);\r\n\r\n    // gather the data from all the nodes to the central node\r\n    MPI_Gather(gcdResults, nodeLength / 2, MPI_INT, results, nodeLength / 2, MPI_INT, 0, MPI_COMM_WORLD);\r\n\r\n    delete[] gcdResults;\r\n    delete[] receivedNumbers;\r\n\r\n    MPI_Finalize();\r\n\r\n    if (worldRank == 0) {\r\n        for (int i = 0; i < resultLength; i++) {\r\n            printf(\"%d\\n\", results[i]);\r\n        }\r\n\r\n        delete[] results;\r\n        delete[] numbers;\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_13.jpg' description='OpenMPI GCD' alt='OpenMPI GCD' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">OpenMPI GCD</p>\r\n</div>\r\n\r\nBesides the output generated by our code, you'll probably remark, in the beginning, some outputs generated by the ssh connection - a sign which confirms that our program runs distributively making use of the other nodes we've specified. \r\n\r\nThis way we can use OpenMPI juice to distribute our program without considering how the data travels between different computers. \r\n\r\nBefore you say that it's a lame example for such a nice tool, I have to admit: it's a lame example for such a nice tool, especially when we talk about real-life challenges. I should've thought of something better, I know, but, what a relief!, [my professor did it again for me..](https://blog.axbg.space/blog/paying-without-money)\r\n\r\nI was talking at the beginning of the article about the course I took on these topics and, like any other course, I also had an interesting homework, to say the least.  \r\n\r\nUsing an already [existing implementation of AES](https://github.com/kokke/tiny-AES-c), I had to implement a C/C++ program that encrypts a file using both multicore processing and distributed nodes - a perfect scenario to mash OpenMP and OpenMPI together! \r\n\r\n### Revisiting AES\r\nThe AES library is *tiny*, easy to use, and includes the most important modes of operation. To keep things as simple as possible, I've chosen ECB, which is the least safe (don't use it in real-world scenarios!), but, what's important for us, the easiest to parallelize, each chunk of 16 bytes being independently encrypted using a secret password.  \r\n\r\nIn terms of padding, the library doesn't provide any implementation, but the author recommends PKCS7, which is simple and safe enough for our playground, so I've implemented it along with the planned encryption. I've talked about it before, so you can take a look [here](https://blog.axbg.space/blog/paying-without-money) for a quick recap.  \r\n\r\nTo preserve readability and to keep the code *tiny*, I've implemented a structure, Block, that holds 16 bytes of data and represents the biggest size for a block that AES can process at once.  \r\n\r\nBesides the actual implementation of a parallel and distributed solution, I wanted to see how it compares to other types of solutions, so I've also written some methods to run AES sequentially and using OMP solely. This way it was easy to benchmark the solutions in the end and draw some conclusions that may or may not be what we were expecting.  \r\n\r\nTo simulate a distributed scenario I spun 2 virtual machines rocking Ubuntu 18.04 on a secondary laptop connected to the same local network.  \r\n\r\nThe current version works only one way, without implementing decryption, because, well, it was not required and I like to think of myself as *very efficient* (but you can call it lazy) \r\n\r\nIn terms of preparing the execution, I've used the same bash script mentioned above, with a few additions to handle input data. I won't spam the article with it, but I'll include it in the code snippets repository. \r\n\r\nThe code is a little bit longer than what we've seen until now, but I added comments on each important line to make the reading more enjoyable. I will also avoid including it here, because it's almost impossible to read it effortlesly without scrolling like a madman. \r\n\r\nWithout further ado, <a href=\"https://github.com/axbg/encapsulated-snippets/blob/main/04%20-%20The%20end%20of%20the%20law/09_aes_sequential_omp_ompi.cpp\" target=\"_blank\">let's take a look!</a>\r\n\r\nThe result of a run looks like this.\r\n\r\n<div class=\"text-center\">\r\n    <img style=\"margin:0 auto\" src='/images/uploads/04_14.jpg' description='AES sequential parallel distributed' alt='AES sequential parallel distributed' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">AES - sequential, parallel & distributed</p>\r\n</div>\r\n\r\nEven if you run the program multiple times or try different kinds of tweaks the result is pretty disappointing. \r\n\r\nFor our example, it's not hard to observe why distributed computing is not something we would like to make use of. Being an approach highly dependent on a network, which should be considered unsafe and unreliable in almost every scenario, distributed computing may not be the best choice to increase performance, but it's viable in a context where some nodes are more powerful than others. \r\n\r\nRegarding the OMP result, we see that it executes slightly slower than the sequential solution. One of the reasons could be the fact that I've used a small input, not more than one hundred characters and, even if most of the time using threads reduces the execution time dramatically, they still require context switches that could be impactful when the processed data is small enough.  \r\n\r\nBesides that, let's not forget we're running our program in a virtual machine which may decrease the overall performance of our multithreaded solution a little bit due to the additional number of actions the hypervisor has to make in order to manage the threads. \r\n\r\nIf I would be executing this program on a 10-years old laptop and the nodes I would be using were represented by bleeding-edge computing beasts, it's easy to guess how the result would change. In another perspective, you need to think for a second about the cloud or any other client-server system and you'll fully understand why, in a distributed system, a big difference of computational power may exist between nodes.\r\n\r\nAs always, programming is about choosing the right tools for the task you want to solve. If the task at hand is learning, as in our case, the best tool remains experimenting.  \r\n\r\nThank you for coming this far! I hope you enjoyed reading the article as much as I've enjoyed writing it and learning more along the way. If you want to run the samples, tweak them or use parts of them in your projects, all the code bits [await you here.](https://github.com/axbg/encapsulated-snippets/tree/main/04%20-%20The%20end%20of%20the%20law)\r\n\r\n<div class=\"text-center\">\r\n    <bold><h3>See you next time</h3></bold>\r\n</div>"
}
