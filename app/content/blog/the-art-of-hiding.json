{
  "publishedAt": "11th May 2020",
  "title": "The Art of Hiding",
  "content": "In many cultures, there's a common saying: a picture is worth a thousand words.  \r\n\r\nSometimes, though, a picture can be worth much more than that.  \r\n\r\nSteganography, the practice of concealing information within an unsuspected object, is, without doubt, one of the most attractive subjects for thriller authors, being a real example that nothing is what it seems.  \r\n\r\nBut, outside these fictional and mystery infused worlds, is steganography a field worth *investigating*?\r\n\r\n#\r\n<div class=\"text-center\">\r\n    <img src='https://images.pexels.com/photos/207681/pexels-photo-207681.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940' description='Looking Glass Picture' alt='Looking Glass Picture' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Photo by <a href='https://www.pexels.com/@pixabay'>Pixabay</a></p>\r\n</div>\r\n\r\n#\r\n### Steganography vs Cryptography\r\nRight of the bat, the answer is **yes**.  \r\n\r\nEven though *The Last Supper* doesn't hide humanity's biggest secret, steganography is a mesmerizing domain, more so if you're combining it with a computer: the thing that we're, actually, doing here.  \r\n\r\nA steganographic object is something that hides a message inside its layers without making it obvious. \r\n\r\nCompared to cryptography, which tries to transform a message in a form that cannot be understood by a middle-man, the aim of\r\nsteganography is to conceal the fact that it carries a message.  \r\n\r\nPretty simple, right?  \r\n\r\nSteganography techniques can also be used in conjunction with cryptography to achieve a higher level of security. \r\n\r\nThink of it like this: if someone analyzes our stego file *(or our embedded cover message, if you want to be more academic)* and observes a human-readable pattern, our secret is compromised. If our secret was encrypted before being embedded in the cover file, the extracted content will be total gibberish and will disguise the fact that a secret exists in the first place.\r\n\r\n#\r\n### Steganography subjects\r\nWhat form factors represent a subject for steganography approaches? Every object that contains information.  \r\n\r\nFor example, long before digital content became ubiquitous, invisible ink was the most common practice of hiding a message. Most of the time, it was applied to letters that displayed a rather dull text, but it could be applied to anything else that can be passed between two parties without raising suspicions for the ones that are aware of the exchange.  \r\n\r\nCould you pass a secret message using your fancy T-Shirt?  \r\nYou bet! [There's a plethora of examples throughout history.](https://en.wikipedia.org/wiki/Steganography###Physical)  \r\n\r\nThe problem was that manipulating physical objects in such a way is hard. They can deteriorate, they can be lost or stolen and a mistake that was made during the secret's insertion made them, basically, useless and materials for building new ones may not have been so easy to gather.  \r\n\r\nFortunately, the digital space made it much easier to generate, correct, store, and share information through multimedia files. Usually, the subjects of digital steganography are the files we encounter daily like bare text, images, sounds, or videos.  \r\n\r\nBased on the spoiler present in the beginning, you've, probably, guessed that today we'll take a closer look at images (pun intended) and observe how we can embed messages into an image and get away with it! \r\n\r\n#\r\n### Our sweet digital images\r\nBefore the *hiding stuff inside images* part, we need to get comfortable with how they are represented inside our devices and which of their aspects are important for what we're trying to do.  \r\n\r\nFirst of all, digital images fall into two major categories: rasters and vectors. \r\n\r\nRasters' main unit is the **pixel** and they are represented as a two-dimensional object: a matrix of pixels.  \r\n\r\nVectors are drawn using complex formulas that describe their shape and can be scaled without losing quality.  \r\n\r\nEven though vectorial steganography is possible, most of the images we use day by day, not to mention the pictures we snap with our smartphones, are rasters.  \r\nToday we'll focus on this omnipresent category and its popular formats.\r\n\r\n#\r\n### From bits to colors\r\nIt would have been simpler for computers if we only used black and white images, but that would have been pretty sad.  \r\nFortunately, we don't, so a way to make computers understand colors was more than needed. You're probably familiar with the most common color models, but, for the record, let's revisit them.   \r\n\r\nFirst, the ever-present **RGB**/**RGBA** which are the models used by all the electronic displays we own today. It's an additive model, meaning that the absence of color is represented the same as our eyes perceive it: as black. RGB uses 3 channels ( plus the alpha channel for RGBA) to represent red, green, and blue. Each color that can be obtained using this model is a sum of each channel multiplied by a factor. A (255, 0, 0) tuple produces a red pixel, (255, 255, 0) a yellow one, and so on. White pops out when we max all the values.  \r\n\r\nThe *additive* qualification expresses how different shades are produced: we start from nothing, we add different shades from our reference colors and we profit.   \r\n\r\nIn counterpart, the subtractive color model or, briefly, **CMY(K)** is used by printers and plays around with cyan, magenta, and yellow to do the exact opposite. From a white background, interpreted by the model as the absolute zero, we start and subtract different values related to our primary colors to create new shades. In the end, when we subtracted everything, the black color results. \r\n\r\nThe value range starts from 0 and goes up to 1, but percentages are commonly used. The K in the acronym stands for key, which is actually black, and it's used to represent black independently.\r\n\r\nWhen its value changes, it darkens the color produced. Outside the digital world, where everything costs, combining all the colors to produce a black shade is considered pricy and the result doesn't look exactly like black, so it's printed independently - that's why you need to buy two cartridges instead of one.  \r\n\r\nBesides these two, which are both graphically represented as a cube, color models that use other tridimensional formats exist. For example, HSL and HSV models are represented using cylinders and utilize their angular dimensions to jump from one shade to another.  \r\n\r\nEven though they're interesting as well, we won't need them today - good old cubes will be more than enough.\r\n\r\n#\r\n### When losing is good\r\nPretty neat until now.  \r\n\r\nA pixel is, actually, a combination of 3 or 4 bytes that, under a convention, has a meaning. The next thing that can become a problem is the fact that images are huge. If you're reading this on a not so high-end phone, your screen resolution is probably 720p, which highly translates to 921600 pixels. \r\n\r\nLet's be optimistic and consider that we only use 3 bytes to represent each one of them. Quick maths: 2.76 megabytes for a quick screenshot. This will count only the raw data of the image. Usually, each format adds some metadata that also takes up space, though not that much.  \r\n\r\nToday our computers have bigger and faster *everything*, but, besides storing or displaying them, images are also sent from one device to another through a connection that may or may not be reliable.  \r\n\r\nFile size matters, even though we hardly think about it when we download 70 gigs to play the latest World of Warcraft. (and wait for Shadowlands to come!)  \r\n\r\nI've mentioned above something related to image formats: JPEG, PNG, BMP - yeah, you know 'em.  \r\n\r\nMost file formats compress the actual data of an image to reduce its size. We're not interested in that topic today. What is interesting for us is that some formats compress the data so much and in such a way that, after the decompression process, the result is not the original image.  \r\n\r\nWhat? Why do they exist then?  \r\nBecause the changes are so subtle that our eyes can't spot the differences between the original version and the one that contains slight changes.  \r\n\r\nWhen we talk about image formats, we say that they are lossy, when information is lost after the image is decompressed, or lossless, when the result of decompression is bitwise identical.  \r\n\r\nThe most common lossy format is **JFIF**, which uses the JPEG compression algorithm, while the most common lossless ones are, probably, **PNG** and **GIF**.  \r\nWhen we're thinking about hiding a message in an image, we need to be sure that the message can be extracted in the same way it was before.  \r\n\r\n[Otherwise bad things may happen](https://www.liveabout.com/thmb/iZW4nCoTZ_Hoz7GLLclTwTI8PQM=/653x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/colon-58b8beae3df78c353c168e0f.jpg).  \r\n\r\nWhile doing so using a lossy format is possible and frequently done, not every steganographic scenario can be applied. To cover a bigger palette of techniques and to avoid the *bad things*, we will use a very familiar lossless format.  \r\n\r\n#\r\n### Hello Bitmap\r\nAnother popular, yet simple, the image format is Bitmap or BMP, created by Microsoft. As expected, is frequently used in Windows, but is far from being endemic. It's safe to say that every major image editor supports BMP.  \r\n\r\nLike PNG, BMP is lossless.  \r\n\r\nUnlike PNG, which features filters, DEFLATE compression and cyclic redundancy checks, BMP's structure is very, very simple and will allow us to play with an image's data without the hassle of doing intermediary operations.  \r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_01.bmp' description='RGB checkerboard' alt=\"RGB checkerboard\" >\r\n</div>\r\n\r\nThis picture represents a 100x100 RGB checkerboard pattern or, in more common slang, a weird chess table stored as a BMP file. The image included here, though, is, actually, four times bigger to avoid unnatural zooming.  \r\n\r\n<div class=\"text-center\">\r\n    <img src='/images/uploads/02_02.jpg' description='Checkerboard structure' alt=\"Checkerboard structure\" >\r\n</div>\r\n\r\nAnd this one displays its content using hexadecimal values, each pair of characters representing 1 byte.  \r\n\r\nSweet and simple, right?\r\n\r\nThe first 10 bytes represent the header of the file which includes BMP's signature and the offset from which the data starts. Image width and image height are specified a little bit further on 4 bytes, in our case, 100, in decimal, for both dimensions. The bits per pixel helps us to identify how many bytes we should read in order to define a pixel.  \r\n\r\nA 24-bits bits BMP has three channels of 8 bits of information.   \r\nA 32-bits have the same three channels plus one more, the alpha channel, that represents the transparency of the current pixel when it's placed over a background.   \r\n\r\nHere, we are in a 24-bits per pixel context, which means one byte for each color of the RGB model or, shall we say BGR?  \r\nThe color scheme of a pixel in BMP is BGR, which is nothing more than a reversed RGB. In our case, for the blue color, the E8h represents the value of the blue channel, followed by A2h yellow and 00h red. \r\n\r\nEven though I tricked you into believing that our checkerboard is composed of red, green, and blue squares, the colors used are not pure. We should remember next time that our eyes are not that good at observing differences between shades. It will help us in a few minutes.  \r\n\r\nBut wait! Our checkerboard starts with a red pixel. Why the first one here is blue?  \r\n\r\nIn BMP rows of pixels are written bottom-up, which means that we described the first two pixels of the last line.\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_03.jpg' description='Checkerboard Last Row' alt=\"Checkerboard Last Row\" >\r\n</div>\r\n\r\nIt is worth mentioning that each line's length should be a multiple of 4, otherwise padding with a variable number of 0 bytes is applied.  \r\n\r\nOur image is 100x100, which means that a line has 100x3 entries. The closest number that is divisible by 4 is exactly 300, so our lines won't be padded at all. If we subtract one pixel from our image, thus obtaining a 99x99 image, we'll gain one padding byte. If we subtract/add two or three more pixels, we'll have two, respectively three padding bytes.  And so on.  \r\n\r\nTo check that what we've assumed is true, let's read a BMP file from our disk. Later we will use a library to reduce the complexity of our code but, for starters, it's a nice exercise to convince ourselves that an image is, in the end, a collection of numbers.  \r\n\r\nFor all the coding we'll be doing today we'll use python, which will save us a lot of time thanks to the vast selection of libraries we can choose from.  \r\n\r\nReady! Set! Let's do this.\r\n\r\n```python\r\nwith open(\"checkerboard.bmp\", \"rb\") as image_file:\r\n    initial_image = image_file.read()\r\n\r\noffset = int.from_bytes(initial_image[10: 13], \"little\")\r\nwidth = int.from_bytes(initial_image[18:21], \"little\")\r\nheight = int.from_bytes(initial_image[22:25], \"little\")\r\n\r\nimage_data = np.ndarray((width, height, 3), np.uint8)\r\n```\r\n\r\nFirst of all, we need to open our image file and read its content, and python has our back. Now, we need to remember the BMP structure we reviewed before and obtain the offset from where our data starts to flow, the width and the height of the image. \r\n\r\nWith this metadata collected, we can declare a 3-dimensional array using NumPy and our freshly read dimensions. We know that each channel can take values in the 0-255 interval, so an 8-bits int data type it's exactly what we need.\r\n\r\n```python\r\nr = c = 0\r\ndata_index = offset\r\nwhile data_index < len(initial_image):\r\n    image_data[r][c][2] = initial_image[data_index]\r\n    image_data[r][c][1] = initial_image[data_index + 1]\r\n    image_data[r][c][0] = initial_image[data_index + 2]\r\n\r\n    c += 1\r\n    data_index += 3\r\n\r\n    if c == width:\r\n        c = 0\r\n        r += 1\r\n\r\n        data_index += width % 4\r\n    \r\nplt.imshow(image_data)\r\nplt.show()\r\n```\r\nAfter that, we can start iterating through our channels and gather data to fill our structure. We will use r and c as indexes for the current row and the current column while moving through the entire data block. \r\n\r\nWe start from the offset, using the data_index variable, and parse until the end, jumping 3 values at a time, one for each channel of our pixel. Because RGB rolls off the tongue much better than BGR, we will populate our structure starting from the last channel to the first and increasing our indexes with each iteration. The column index will increase by 1, while the *data_index* with 3.  \r\n\r\nIf we reach the end of a row, we increment our column counter and reset the row index - we're ready for a new round.  \r\n\r\nBut not yet though.  \r\n\r\nEven if we're lucky enough to have a row length divisible with 4, sometimes this may not be the case. When we reach the end of a row we need to check our row length against 4. Based on the remainder, we need to increment the data_counter to ensure that we'll not trip over padding bytes.  \r\n\r\nIn the end, to let our eyes be the final judges, we rely on pyplot to display our image, but before that, we need to flip our structure because we read it from the bottom to top.  \r\n\r\n#\r\n### Destination: Steganography\r\nNow that images are not such a big mystery anymore, steganography starts to seem like something doable, even though we still have some intense topics ahead.  \r\n\r\nEvery time we're talking about software, the amount of things that can be done in just one way strives to zero. There's always another way of doing it and this is valid for digital steganography as well.  \r\n\r\nDigital steganography techniques fall into two categories, based on the domain in which the cover object is represented.  \r\nWe can talk about  \r\n  * Spatial Domain Techniques\r\n  * Frequency Domain Techniques\r\n\r\nWe'll tackle each one separately in order to understand and implement them in a common scenario.\r\n\r\n#\r\n### Spatial Domain Techniques\r\nWe'll start with the obvious family of techniques, the ones applied to the spatial domain of the image.  \r\n\r\nEven though math concepts can sound pretty complex in English, spatial domain refers to the structure of the image we've already met before: its matrix representation.  \r\n\r\nThese techniques modify some values from one or more channels while trying to minimize the impact on the appearance of the cover image. They are based on the fact that the human eye cannot perceive the subtle difference between two similar colors. \r\n\r\nThe most popular steganography techniques that operate on the spatial domain of an image are:  \r\n- **Least Significant Bit (LSB)** - which makes a profit on the fact that changes applied on the least significant bit will have a small impact on the total value of the byte - the difference in color that results is imperceptible and can not be caught by the human eye  \r\n- **Pixel Value Differencing (PVD)** - usually applied on greyscale images, but not only, PVD uses two consecutive pixels, computes their difference and, based on a comparison with a reference table, a variable quantity of the secret is embedded in each pixel (a marvelous explanation and adaptation for colored images can be found [here](https://royalsocietypublishing.org/doi/full/10.1098/rsos.161066))\r\n- **Bit Plane Complexity Segmentation (BPCS)** - segments the cover image in n bit planes, each plane being a matrix of bits that occupy the same position inside the bytes their extracted from and tries to embed the message, usually a secret image, that is decomposed in the same way, throughout the complex regions of each bit-plane - complexity computation and secret image transformation are nicely explained in [this paper](https://www.researchgate.net/publication/50346770_Review_Steganography_-_Bit_Plane_Complexity_Segmentation_BPCS_Technique)\r\n\r\n#\r\n### Taming LSB\r\nThe most common spatial domain technique is the **Least Significant Bit** approach. \r\n\r\nDespite being the simplest in the tool belt, the embedded image produced by LSB is very similar to the original one, making an unsuspicious viewer continue his day as nothing happened.  The major problems of LSB are its low hiding capacity and robustness. \r\n\r\nWe chose BMP, a non-compressed format because we could do so. In a real-life scenario, this might not be the case, and the image might suffer conversions from a format to another.  \r\n\r\nIf we transform a BMP image into a PNG the message will become unretrievable as long as the image is not converted back. This is, nevertheless, a happy scenario, but, if one lossy format enters the scene, the information may or may not be lost, depending on which bytes are exactly preserved and which are slightly changed. If we try to deliver an important message, any degree of uncertainty should be avoided.  \r\n\r\n#\r\n### Writing down the secret\r\nThe image of the weird chess table we used before has 100x100x3 bytes of data. This means that using LSB, we can hide, at best, 240000 bits and hope that nobody will notice. \r\n\r\nIn this case, our message should have, at max, 3750 ASCII characters if we want to profit from the whole available space. This means to change almost each least significant bit in our image!  \r\n\r\nOr maybe not.  \r\n\r\nThe bit has 2 possible states: 0 or 1. It's rational to assume that some of the bits we will try to embed have the same value as the ones that already exist in the image, right?  \r\n\r\nSo, even if our message will be embedded in the cover image, the number of bits that will change will be smaller than the actual size of the message. \r\n\r\nThis sounds like good news, but it's not such a big deal. Even if we switch each least significant bit, human eyes won't notice any difference at all.  \r\n\r\nEither way, 3750 characters is enough for us to hide a fancy secret in it.  \r\nAnd what can be more *secret* than the first [4 paragraphs of Lorem Ipsum](https://github.com/axbg/encapsulated-snippets/blob/master/02%20-%20The%20art%20of%20hiding/lorem) generated by [lipsum.com](https://www.lipsum.com/)? \r\n\r\nSummed up, we will embed 3131 bytes in our cover image.  \r\n\r\nTo make it harder for eavesdroppers to intercept our message, let's add a little more complexity, shall we?  \r\n\r\nBefore embedding our message, we'll split it into the whole available space using the concept of planes we learned before. Instead of embedding our secret consecutively in each pixel's channels, we will spread the first third of our secret in the red plane and so on. \r\n\r\nBesides the algorithm, the message extraction procedure will also require the length of our message. Guessing its size will make the procedure harder, but still possible.  \r\n\r\nWill this approach greatly improve the result? [No.](https://i.imgflip.com/2yvmo3.jpg)  \r\nWill it be fun to implement? Of course!  \r\n\r\nAn additional useful procedure would be to apply an XOR between the message and a password chosen by us, adding, this way, some nitty-gritty cryptography that will protect the content of our steganographic object. We won't do that, though, because we'll exceed the borders of our current topic.   \r\n\r\nNow let's proceed and do some computer magic.  \r\n\r\n```python\r\nwith open(\"lorem\", \"rb\") as lorem_file:\r\n    lorem = lorem_file.read()\r\n\r\nimg = cv2.imread(\"checkerboard.bmp\", cv2.IMREAD_COLOR)\r\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n\r\nif len(lorem) * 8 > img.size * 0.8:\r\n    print(\"Cover image is not big enough\")\r\n    exit(-1)\r\n\r\none_third_size = int(len(lorem) / 3)\r\n\r\ngreen_message_size = blue_message_size = one_third_size\r\nred_message_size = one_third_size if len(lorem) % 3 == 0 else one_third_size + (len(lorem) % 3)\r\n\r\nembed_message(img[:, :, 0], lorem, 0, red_message_size)\r\nembed_message(img[:, :, 1], lorem, red_message_size, green_message_size)\r\nembed_message(img[:, :, 2], lorem, red_message_size + green_message_size, blue_message_size)\r\n\r\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\r\ncv2.imwrite(\"lsb_result.bmp\", img)\r\n```\r\n\r\nWe'll start by reading the files we'll work with.  \r\n\r\nAs I've said after the previous exercise, we will rely on OpenCV to correctly import our image. We'll also switch the BGR order to RGB. \r\n\r\nNext, we need to check if our image is big enough to become our cover object, by checking if the number of bits contained in our message is smaller than 80% of our image. Depending on the colors and the shapes captured, the ratio can be increased or decreased, until the result is satisfactory.  \r\n\r\nThen, we'll compute how much information we can cram in each channel in the happy and improbable scenario of a perfect division between the length of our secret and the number of our channels. I hardcoded \"3\" because, in our small example, we're using an image that doesn't have an alpha channel. I chose to add the remainder to the red channel, but it doesn't matter which one is chosen.   \r\n\r\nThe last steps include calling a method, embed_message, for each one of our channels. switching our image back in BGR format and saving it on the disk.  \r\n\r\nThe last piece we need to discuss is the magical *embed_message* method.\r\n\r\n```python\r\ndef embed_message(channel, message, offset, length):\r\n    rows, cols = channel.shape\r\n    \r\n    bits_per_row = int(length * 8 / rows)\r\n    \r\n    space_between_bits = int(cols / bits_per_row)\r\n    \r\n    actual_message = message[offset:offset + length]\r\n\r\n    outer_index = inner_index = 0\r\n\r\n    for i in range(0, rows):\r\n        for j in range(0, cols, space_between_bits):\r\n            if outer_index == length:\r\n                break\r\n\r\n            current_letter_bits = bin(actual_message[outer_index])[2:].zfill(8)\r\n            current_letter_bit = current_letter_bits[inner_index]\r\n\r\n            current_channel_bits = bin(channel[i][j])[2:].zfill(8)\r\n            current_channel_bits = current_channel_bits[:7] + current_letter_bit\r\n\r\n            channel[i][j] = int(current_channel_bits, 2)\r\n\r\n            if inner_index == 7:\r\n                inner_index = 0\r\n                outer_index += 1\r\n            else:\r\n                inner_index += 1\r\n\r\n    print(\"Inserted {} bytes\".format(length))\r\n    print(\"Current offset inside message {}\\n\".format(outer_index)) \r\n```\r\n\r\nThe method receives the content of a channel, the full message, the offset inside the message from where we should start reading, and the length of the segment we should process. \r\n\r\nTo make the implementation a little bit more visual, I tried to use as many self-describing variables I could. I love one-liners, but many times they're not quite love at first sight.  \r\n\r\nWe start by computing all the sizes we'll need the number of rows and columns of our channel and the space between insertions. \r\n\r\nTo start our parsing from 0, we extract in the actual_message the chunk of data we need to hide. The outer_index variable will be used to track on which character in the message we're currently operating, while inner_index on which bit inside the above-named char.  \r\n\r\nThe two iterations will cover the entire space of the matrix while skipping values on the horizontal dimension to ensure that our insertions are not consecutively done. \r\n\r\nThe break inside the loops is used for the last line in the channel. Because mathematics is not solely based on integers, the last element we insert may not fall exactly on the last element in the matrix. Without the additional conditions, the program would *break* (pun intended) most of the time.  \r\n\r\nInside the for loops, we begin juggling with bits. We unpack the current character in the message into bits. Python strips unnecessary zeroes, thus we need to pad it to make sure that each of our steps executes in the same way.  \r\n\r\nThen, we extract the current bit we need to handle from the current character and replace the least significant bit in the channel byte we've landed on.  \r\n\r\nAfter packing the bits back into a byte, the last thing we have to do is to determine the displacement for the next step. If the inner_index variable has not reached 7 yet, meaning that there are more unembedded bits left in our character, we simply move to the next one, but, if we just processed the last bit, we need to move on to the next character and set the inner_index on the beginning. \r\n\r\nIn the end, for the sake of logging, we print how many characters we successfully embedded in the processed channel.  \r\n\r\nAfter so much effort, let's see if the result matches our expectations by comparing the two images using the same scaling as before.\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto'  src='/images/uploads/02_01.bmp' description='Original Picture' alt='Original Picture' >\r\n    <p style='margin-top:0px;font-size:12px;'>Original</p>\r\n</div>\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto'  src='/images/uploads/02_04.bmp' description='Result Picture' alt='Result Picture' >\r\n    <p style='margin-top:0px;font-size:12px;'>Result</p>\r\n</div>\r\n\r\nWhen we put the original image toe to toe with our stego result, we cannot see any difference at all. This is pretty fascinating if we consider that our cover image has 100x100 pixels and it's scaled by a factor of four!  \r\n\r\nWe cannot count on our eyes for complex patterns. Even more interesting is the fact that we produced something this good using LSB, the most intuitive and simple steganography technique.   \r\n\r\nThis seems too good to be true and, for a scenario when nobody decides to analyze our image, it really is. However, besides steganography, there's also a thing called *steganalysis* which does exactly the opposite: it tries to find if a file contains a secret and retrieve it.   \r\n\r\nSteganalysis is an art in itself and I'm not familiar at all with its methods, but one of the simplest approaches is to analyze the color histogram of our images.  \r\nThe histograms were generated using [Lunapic](https://www4.lunapic.com/editor/).\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_05.jpg' description='Checkerboard Histogram' alt='Checkerboard Histogram' >\r\n    <p style='margin-top:0px;font-size:12px;'>Checkerboard Histogram</p>\r\n</div>\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_06.jpg' description='LSB Histogram' alt='LSB Histogram' >\r\n    <p style='margin-top:0px;font-size:12px;'>LSB Histogram</p>\r\n</div>\r\n\r\nWe can observe that, even though they seem identical, the cover image and the LSB embedded one have different color structures. The fact that our stego is shadier than our eyes tend to admit can be easily remarked if we compare the two histograms. \r\n\r\nLSB may be good enough to fool our eyes, but steganalysis is way harder to trick.  \r\n\r\nWe need something better and, thanks to some brilliant mathematicians, especially Fourier and Nasir Ahmed, we got exactly what we're asking for.  \r\n\r\n### Transform Domain Steganography\r\nCompared to the spatial domain, which describes an image the way we perceive it, the frequency domain, the environment in which transform domain steganography operates, is way more abstract.  \r\n\r\nI mentioned Fourier earlier not because I'm into mathematics, but because, using the Fourier transform, we can express the components of images based on their frequencies, rather than their position and color. \r\n\r\nI know, it seems tricky and it took me a while to internalize what all of this means. And, even though I can't say that I've got it a hundred percent right, here are the steps that I've taken during my research that helped me to grasp the concept.  \r\n\r\nIf you don't have an engineering background, which I gracefully don't, you're not playing every day with frequencies - even less with frequencies that describe a two-dimensional object, such as a picture.  \r\n\r\nFor a second, let's talk about sounds.  \r\nA sound is a wave. The classical approach to describe a wave uses amplitude and time, which looks like this.\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='https://upload.wikimedia.org/wikipedia/commons/2/21/4-bit-linear-PCM.svg' description='' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Published on Wikipedia</p>\r\n</div>\r\n\r\nThe amplitude of a soundwave expresses how loud the sound it appears to be, but offers nothing more to the table.  \r\n\r\nApplying the Fourier transform, which can be done using a [not so pretty looking formulas](https://en.wikipedia.org/wiki/Fourier_transform), we can express, the same signal in its frequency domain, a process that adds a little more information to our graphs.\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='https://upload.wikimedia.org/wikipedia/commons/7/72/Fourier_transform_time_and_frequency_domains_%28small%29.gif' description='' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Published on Wikipedia</p>\r\n</div>\r\n\r\nThe relation between the two representations is nicely captured in this animation, which made me understand how this works for sounds and other unidimensional continuous signals, but not for images, which are both two-dimensional and finite. \r\n\r\nLater on, I've stumbled upon another formula maths magic, called the [Discrete Fourier Transform](https://en.wikipedia.org/wiki/Discrete_Fourier_transform), which describes how we can produce the same mapping for a discrete signal. The result of DFT is composed of multiple pairs of a real and a complex number and the image can be converted back using the inverse function, without damaging its structure.  \r\n\r\nSignals are nice, frequencies are cool, but it's hard to observe a direct resemblance between a picture of a cat and a sine wave. The thing is that any image, no matter how complex, can be represented as a sum of waves with different frequencies. [This video](https://www.youtube.com/watch?v=mEN7DTdHbAU) explains it way better than I can, so check it out if you feel that the aforementioned sentence seems weird.  \r\n\r\nEven though spectral representations produced by DFT offer a nice amount of information, working with DFT, which relies on complex numbers, is not always a pleasure.  \r\n\r\nBased on the Fourier Transform, an electrical engineer, Nasir Ahmed, invented the [Discrete Cosine Transform](https://en.wikipedia.org/wiki/Discrete_cosine_transform) which, as you've probably guessed, uses cosine as its base function and produces real coefficients. DCT is frequently used in data compression, JPEG being the perfect example.  \r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_07.jpg' description='Lenna' alt='Lenna' >\r\n    <p style='margin-top:0px;font-size:12px;'>Lenna</p>\r\n</div>\r\n\r\n<div class=\"text-center\">\r\n<img style='margin:0 auto' src='/images/uploads/02_08.jpg' description='DFT' alt='DCT' >\r\n    <p style='margin-top:0px;font-size:12px;'>DFT</p>\r\n</div>\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_09.jpg' description='DCT' alt='DCT' >\r\n    <p style='margin-top:0px;font-size:12px;'>DCT</p>\r\n</div>\r\n\r\nNow, after we went through the complex concepts that will help us further, I invite you to meet Lenna, the equivalent of cryptography's Bob & Alice in image processing. \r\n\r\nThe next two images are nothing else than her frequency domain representations obtained using DFT and DCT (applied on blocks of 8x8 pixels).  \r\nThis is exactly how my depiction of bad TV signal looks like. In this case, though, it means a little bit more than that.   \r\n\r\nYou've probably noticed that, unlike our image, Lenna is black and white. Both DFT and DCT can be applied to two-dimensional objects, such as a matrix, but cannot be applied directly on a colored image which is stored as three different matrices. \r\n\r\nIn our example, we'll use DCT on a colored image by separating the color planes and process each one independently.   \r\n\r\nBefore we jump to coding, we have to talk a little bit more about frequencies and the frequency domain representations.  \r\n\r\nFirst of all, the frequency level represents how fast the intensity of pixels changes throughout an image. \r\n\r\nA picture with a single color has the frequency close to 0 because there's no variation. A picture with a gradient will have a small frequency, because the changes between shades happen gradually, while a picture with black and white pixels placed consecutively will have a big frequency because the variation of intensity that appears between two neighbor pixels is maximum.   \r\n\r\nWhen we talk about regular images, high-frequency regions are represented by prominent edges and very detailed components, while low-frequency regions are captured by uniform colored areas.  \r\n\r\nThe DFT representation is commonly used for spectral analysis. Understanding it will help us to understand what we can achieve by playing with frequencies.  \r\n\r\nIn Lenna's DFT representation, the zero-frequency component is placed in the center of the image. From black to bright white, from low to high, each pixel in the spectrum shows how much a specific two-dimensional wave pattern is involved in the process of creating the original image. \r\n\r\nA regular picture is represented using mostly low-frequency components, so we expect to see a bright core. The further from the center we are, the higher the frequency.  \r\n\r\nIn simpler terms, for DFT, the corners represent edges and the center smooth surfaces.  \r\n\r\nDCT representation has a similar interpretation, even though their pictures don't look alike. In DCT, the zero-frequency point is represented in the upper-left corner, while the areas in the opposite direction capture the high-frequency components. \r\n\r\nLenna's DCT looks this confusing because DCT was applied for each 8x8 block of pixels from the original image. The resultant coefficients replaced the pixels in the final output. The bigger the coefficient, the whiter its pixel will appear.  \r\n\r\nWe can distinguish the blocks that have a higher frequency in the sections representing Lenna's contour.   \r\n\r\nWhen the inverse method is applied, each coefficient is multiplied with the patterns captured in the following image, based on its position inside the matrix.  \r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto; max-height: 300px;' src='https://upload.wikimedia.org/wikipedia/commons/6/63/Dct-table.png' description='' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Published on Wikipedia</p>\r\n</div>\r\n\r\nAfter this operation is done, we should obtain a representation that closely resembles our initial picture.  \r\n\r\nYep, sometimes math is truly magical.  \r\n\r\nIn terms of image processing, the frequency domain representation is golden, because, using this form, many properties of the original picture can be altered without directly touching the pixels. For example, we can apply a blur filter if we tune out the values of the coefficients placed near the edges in a DFT representation.  \r\n\r\nSteganography tries to employ the same concepts: embedding our message without altering individual pixels, but spreading it throughout the image by changing the coefficients of its DCT representation.  \r\n\r\nWe've gone through a lot of theory so far.  \r\nIt's time to put it into practice.   \r\n\r\n#\r\n### Scrambling coefficients\r\nAs I've mentioned earlier, the Discrete Cosine Transform will be very useful starting from now.  \r\n\r\nWe'll start by splitting our cover image's channels into 8x8 blocks. It doesn't matter if our dimensions are not divisible with 8, but it's important to be consistent during the embedding process. We'll simply skip the remaining bits that don't fit in such a block and we'll move downwards to the next line of matrices.  \r\n\r\nThen, we'll apply DCT on each block and that's when we hit our first question: Where should we embed the bytes of our secret?   \r\n\r\nThe most sensitive areas in terms of human perception are the ones dominated by high frequencies. \r\n\r\nIf we try to embed our bytes here, the cover image will gain some features that will draw attention upon them.  \r\n\r\nIf we try to use the upper left corner, which describes low frequencies, there's a big chance that we'll alter the structure of our initial cover as, in general, a picture is mostly \"built\" of low-frequency areas.  \r\n\r\nA simple answer that will be good enough for us is to alter a coefficient that describes a medium frequency pattern. We will replace the coefficient placed on the fifth row, fifth column position inside our DCT matrix with one byte of data, but not before we try to lower the value stored in our byte to reduce the impact we'll produce. \r\n\r\nFirst, as we know that one byte can take values between 0-255, we'll start by centering the interval around 0. This way our coefficients, even though they might become negative, will have a lower amplitude.  \r\n\r\nAfter that we multiply the result with a factor chosen by us, let's say 0.8, to reduce its value.  \r\n\r\nTo have a bigger space of maneuver, we'll choose a bigger image as our cover. Let's say this one, which has a resolution of 1920x1280 pixels:\r\n\r\n<div class=\"text-center\">\r\n    <img src='https://images.unsplash.com/photo-1506422748879-887454f9cdff?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80' description='Skyline' alt='Skyline'>\r\n    <p style=\"margin-top:0px;font-size:12px;\">Photo by <a href='https://unsplash.com/@fezbot2000'>Fezbot2000</a></p>\r\n</div>\r\n\r\n To shift the perspective a little bit, we'll give up on our previous secret, the Lorem Ipsum paragraphs, and we'll try to embed something more similar to the cover itself: another image - nothing else than a 100x100 colored picture of Lenna.\r\n\r\n ```python\r\nimg = cv2.imread(\"cover.bmp\", cv2.IMREAD_COLOR)\r\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n\r\nsecret_image = cv2.imread(\"checkerboard.bmp\", cv2.IMREAD_COLOR)\r\nsecret_image = cv2.cvtColor(secret_image, cv2.COLOR_BGR2RGB)\r\n\r\ncover_height, cover_width, _ = img.shape\r\n\r\npossible_matrices = int(cover_width / 8) * int(cover_height / 8)\r\n\r\nif possible_matrices < secret_image.size / 3:\r\n    print(\"Secret image is too big for the cover image you want to use\")\r\n    exit(-1)\r\n ```    \r\n\r\n As always, we start by loading our assets and checking if the space available in the cover image is enough to embed the chosen secret.  \r\n\r\n Here, because we will use one block of 8x8 pixels to hide one byte of data, we check if we can come up with enough matrices to fit the entire message.  \r\n\r\n ```python\r\nnorm = 0.8\r\nembed_channel(img[:, :, 0], secret_image[:, :, 0], norm)\r\nembed_channel(img[:, :, 1], secret_image[:, :, 1], norm)\r\nembed_channel(img[:, :, 2], secret_image[:, :, 2], norm)\r\n\r\nretrieved_secret_image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\r\ncv2.imwrite(\"embedded_cover.bmp\", retrieved_secret_image)\r\n ``` \r\n\r\n We proceed by choosing a satisfactory norm, used in the normalization process. If the value is too big, the cover image may be visibly changed. If it's too small, the bytes we'll extract will be slightly different than the ones we've inserted. \r\n \r\n This danger is still present, though, as the inverse DCT will not produce an identical reconstruction of our secret, but a form that's good enough to resemble the embedded object.  \r\n \r\n The hiding process is implemented in the embed_channel method, which will map the same channels between the secret and the cover image.\r\n\r\n ```python\r\ndef embed_channel(channel, hidden_channel, norm):\r\n    height, width = channel.shape\r\n\r\n    hidden_channel_list = [item for sublist in hidden_channel.tolist() for item in sublist]\r\n\r\n    outer_index = 0\r\n    for i in range(0, height, 8):\r\n        for j in range(0, width, 8):\r\n            if j + 7 >= width or outer_index == len(hidden_channel_list):\r\n                break\r\n\r\n            dct_block = scipy.fft.dct(scipy.fft.dct(channel[i:i + 8, j:j + 8].T, norm='ortho').T, norm='ortho')\r\n\r\n            dct_block[4][4] = (hidden_channel_list[outer_index] - 127) * norm\r\n\r\n            channel[i:i + 8, j:j + 8] = scipy.fft.idct(scipy.fft.idct(dct_block.T, norm='ortho').T, norm='ortho')\r\n\r\n            outer_index += 1\r\n ``` \r\n\r\nTo simplify the parsing, we transform the hidden_channel matrix in a list and we track the current element using the outer_index variable. \r\n\r\nAs I've mentioned before, if our image cannot be split exactly into 8x8 blocks, we'll ignore the pixels placed at the end of the rows. That's why we need to break the inner loop when we exceed the available space or, as our message will, probably, not be divisible with the number of matrices we can fit in an 8x8 row when we've hidden all the available information.  \r\n\r\nInside the loops, we compute the DCT form of each block using SciPy. Naturally, DCT works with unidimensional arrays, so we have to apply it two times for each dimension. [Here's](https://stackoverflow.com/questions/40104377/issiue-with-implementation-of-2d-discrete-cosine-transform-in-python) a nice response that describes how and why we should use it like that.  \r\n\r\nAfter we've obtained the coefficients, we center one byte of our image around 0, multiply it with the norm we've chosen before, and place the result in the fifth row, the fifth column in the resultant matrix. \r\n\r\nIn the last step, we translate the frequency-based representation back into the spatial domain by applying the inverse DCT function and store the result in the same positions we've extracted the data from. \r\n\r\nThen, we move to the next secret byte, and so on.  \r\n\r\nAfter so much work, we'd better get something nice in return! \r\n\r\nHere's the result of what we've implemented.\r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='https://images.unsplash.com/photo-1506422748879-887454f9cdff?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80' description='Original' alt='Original'>\r\n    <p style=\"margin-top:0px;font-size:12px;\">Original</p>\r\n</div>\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_10.jpg' description='Result' alt='Result' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Result</p>\r\n</div>\r\n\r\nWe've already concluded that our eyes can be easily deceived, but we have to admit, though, that the result looks pretty good from afar. \r\n\r\nIt's important, to be frank: frequency-based steganography is not a piece of cake and our algorithm is very simple. We've barely scratched the surface here and, if we take a closer look, you're going to see what I'm talking about.  \r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_11.jpg' description='Zoomed Result' alt='Zoomed Result' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Zoomed Result</p>\r\n</div>\r\n\r\nThe pattern representing the coefficient we altered is spottable even with all our efforts to minimize the impact!  \r\n\r\nIf you try to hide something from your neighbor, nobody will notice, but if you're hiding vital information you'll probably need to do a little bit more research.  \r\n\r\nWe can take a look at the color histograms, just like we did before, and observe that, despite being very subtle, the impact still exists. \r\n\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_12.jpg' description='Original Histogram' alt='Original Histogram' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Original Histogram</p>\r\n</div>\r\n<div class=\"text-center\">\r\n    <img style='margin:0 auto' src='/images/uploads/02_13.jpg' description='Result Histogram' alt='Result Histogram' >\r\n    <p style=\"margin-top:0px;font-size:12px;\">Result Histogram</p>\r\n</div>\r\n\r\nIf we consider the long route we took to produce this example, LSB may seem like a sweeter approach, being easier to implement and understand, but depending on the quality and the structure of the cover image, as well as the size and type of the secret message, you can achieve a better result using a frequency flavored technique. \r\n\r\nAs with everything else, it depends.  \r\n\r\nSteganography and steganalysis are complex practices and, even after such a long description, we barely covered the basics. Lots of papers are available online, covering methods with different levels of depth.  \r\n\r\nI hope that you, too, perceive steganography as a more tangible subject after reading this article. I, for sure, learned a lot while writing it.  \r\n\r\nThe code I've used and, also, the extraction programs that I've teased are available [here](https://github.com/axbg/encapsulated-snippets/tree/master/02%20-%20The%20art%20of%20hiding).  \r\n\r\nThis was, indeed, a very long article, but I hope it was worth it.  \r\n\r\n<div class=\"text-center\">\r\n    <bold><h3>See you next time</h3></bold>\r\n</div>",
  "tag": "General Concepts",
  "seoDescription": "In many cultures, there's a common saying: a picture is worth a thousand words. Sometimes, though, a picture can be worth much more than that.",
  "seoMetaImage": "https://images.pexels.com/photos/207681/pexels-photo-207681.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940"
}